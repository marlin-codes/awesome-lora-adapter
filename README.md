## Paper List of Low-Rank Adapters (Updating)

### 1. Basic LoRA Adapters

#### a. Adapting Weights of LoRA Adapters
- Asymmetry in Low-Rank Adapters of Foundation Models | [arXiv 2402](https://arxiv.org/abs/2402.16842)
- LoRA-FA: Memory-efficient low-rank adaptation for large language models fine-tuning | [arXiv 2308](https://arxiv.org/pdf/2308.03303.pdf)
- FLoRA: Low-Rank Adapters Are Secretly Gradient Compressors | [arXiv 2402](https://arxiv.org/pdf/2402.03293.pdf) |  [Code](https://github.com/MANGA-UOFA/FLoRA)
- LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models | [arXiv 2402](https://arxiv.org/pdf/2402.11417.pdf) |  [Code](https://github.com/yifanycc/loretta)
- BiLoRA: A Bi-level Optimization Framework for Low-rank Adapters | [ICLR](https://openreview.net/pdf?id=Svy1XoOLXj)
- Krona: Parameter efficient tuning with kronecker adapter | [arXiv 2212](https://arxiv.org/pdf/2212.10650.pdf)
- AutoLoRA: A parameter-free automated robust fine-tuning framework | [arXiv 2310](https://arxiv.org/pdf/2310.01818.pdf)
- LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation | [arXiv 2402](https://arxiv.org/pdf/2402.07721.pdf)

#### b. Low-Ranking Strategies in LoRA Adapters
- Sparse Low-rank Adaptation of Pre-trained Language Models | [arXiv 2311](https://arxiv.org/pdf/2311.11696.pdf) |  [Code](https://github.com/TsinghuaC3I/SoRA)
- ADAPTIVE BUDGET ALLOCATION FOR PARAMETER EFFICIENT FINE-TUNING | [ICLR 2023](https://openreview.net/pdf?id=lq62uWRJjiY)
- ReLoRA: High-Rank Training Through Low-Rank Updates | [arXiv 2307](https://arxiv.org/pdf/2307.05695.pdf) |  [Code](https://github.com/guitaricet/reLoRA)
- QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning | [arXiv 2402](https://arxiv.org/pdf/2402.10462.pdf)
- PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation | [arXiv 2401](https://arxiv.org/pdf/2401.11316.pdf)
- Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning | [Mathematics 2023](https://go.gale.com/ps/i.do?id=GALE%7CA772098727&sid=sitemap&v=2.1&it=r&p=AONE&sw=w&userGroupName=a04fu&aty=ip)
- Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning | [arXiv 2401](https://arxiv.org/pdf/2401.04151.pdf)
- PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA | [arxiv 2402](https://arxiv.org/abs/2402.16902)
- DoRA: Weight-Decomposed Low-Rank Adaptation | [arXiv 2402](https://arxiv.org/pdf/2402.09353.pdf)
- Delta-LoRA: Fine-tuning high-rank parameters with the delta of low-rank matrices | [arXiv 2309](https://arxiv.org/pdf/2309.02411.pdf)

#### c. Learning Rates for LoRA Adapters
- LoRA+: Efficient Low-Rank Adaptation of Large Models | [arXiv 2402](https://arxiv.org/pdf/2402.12354.pdf) |  [Code](https://github.com/nikhil-ghosh-berkeley/LoRAplus)

#### d. Dropout for LoRA Adapters
- LoRA Meets Dropout under a Unified Framework

#### e. Learning Strategies for LoRA Adapters
- Batched Low-Rank Adaptation of Foundation Models | [arXiv 2312](https://arxiv.org/pdf/2312.05677.pdf)
- Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning | [arXiv 2401](https://arxiv.org/pdf/2401.04151.pdf)
- AMAL: Meta Knowledge-Driven Few-Shot Adapter Learning | [ACL 2022](https://aclanthology.org/2022.emnlp-main.709.pdf)
- PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA | [arxiv 2402](https://arxiv.org/abs/2402.16902)

#### f. Post-hoc Processing for LoRA Adapters
- Bayesian Low-rank Adaptation for Large Language Models | [arXiv 2308](https://arxiv.org/abs/2308.13111)

#### g. Deployment of LoRA Adapters
- S-LoRA: Serving thousands of concurrent LoRA adapters | [arXiv 2311](https://arxiv.org/pdf/2311.03285.pdf) |  [Code](https://github.com/S-LoRA/S-LoRA)
- CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference | [arXiv 2401](https://arxiv.org/pdf/2401.11240.pdf)
- Local LoRA: Memory-Efficient Fine-Tuning of Large Language Models | [OpenReview](https://openreview.net/pdf?id=LHKmzWP7RN#:~:text=Our%20approach%20aims%20to%20decouple,LoRA%20on%20math%20reasoning%20tasks.)
- Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning | [arXiv 2401](https://arxiv.org/pdf/2401.04151.pdf)
- VeRA: Vector-based Random Matrix Adaptation | [ICLR 2024](https://openreview.net/forum?id=NjNfLdxr3A)
   
### 2. Advanced LoRA Adapters

#### a.Advanced LoRA Adapters with Improved Structures
- Hydra: Multi-head low-rank adaptation for parameter efficient fine-tuning | [arXiv 2309](https://arxiv.org/pdf/2309.06922.pdf) |  [Code](https://github.com/extremebird/Hydra)
- LoRARetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild | [arXiv 2402](https://arxiv.org/pdf/2402.09997.pdf)
- LoRAhub: Efficient cross-task generalization via dynamic LoRA composition | [arXiv 2307](https://arxiv.org/pdf/2307.13269.pdf) |  [Code](https://github.com/sail-sg/LoRAhub)
- One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning | [arXiv 2306](https://arxiv.org/pdf/2306.07967.pdf) |  [Code](https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA)
- LoTR: Low Tensor Rank Weight Adaptation | [arXiv 2402](https://arxiv.org/pdf/2402.01376.pdf) |  [Code](github.com/skolai/lotr)

#### b. Advanced LoRA Adapters with Ensembles

- LoRA ensembles for large language model fine-tuning | [arXiv 2310](https://arxiv.org/pdf/2310.00035.pdf)
- Ensemble of low-rank adapters for large language model fine-tuning | [arXiv 2310](https://arxiv.org/pdf/2310.00035.pdf)
- Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning | [arXiv 2402](https://arxiv.org/abs/2402.17263)


#### c. Advanced LoRA Adapters with Mixture of Experts
- Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning.| [arXiv 2309](arXiv:2309.05444(2023))
- MoeLoRA: An moe-based parameter efficient fine-tuning method for multi-task medical applications | [arXiv 2310](https://arxiv.org/pdf/2310.18339.pdf) | [Code](https://github.com/liuqidong07/MOELoRA-peft)
- LoRAMOE: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment | [arXiv 2312](https://arxiv.org/abs/2312.09979)
- LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs | [arXiv 2401](https://arxiv.org/pdf/2401.16160.pdf)
- MoeLoRA: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models | [arXiv 2402](https://arxiv.org/pdf/2402.12851.pdf)
- Higher Layers Need More LoRA Experts | [arXiv 2402](https://arxiv.org/pdf/2402.08562.pdf) |  [Code](https://github.com/GCYZSL/MoLA)

#### d. Advanced LoRA Adapters with Quantization
- QLoRA: Efficient finetuning of quantized llms | [arXiv 2305](https://arxiv.org/pdf/2305.14314.pdf) | [Code](https://github.com/artidoro/qLoRA)
- Qa-LoRA: Quantization-aware low-rank adaptation of large language models | [NeurIPS 2023 (oral)](https://arxiv.org/pdf/2309.14717.pdf) |  [Code](https://github.com/yuhuixu1993/qa-LoRA)
- Loftq: LoRA-fine-tuning-aware quantization for large language models | [arXiv 2310](https://arxiv.org/pdf/2310.08659.pdf) |  [Code](https://github.com/yxli2123/LoftQ)
- Lq-LoRA: Low-rank plus quantized matrix decomposition for efficient language model finetuning | [arXiv 2311](https://arxiv.org/pdf/2311.12023.pdf) |  [Code](https://github.com/HanGuo97/lq-LoRA)
- LQER: Low-Rank Quantization Error Reconstruction for LLMs | [arXiv 2402](https://arxiv.org/pdf/2402.02446.pdf)
- 
#### e. LongLoRA
- LongLoRA: Efficient fine-tuning of long-context large language models | [arXiv 2309](https://arxiv.org/pdf/2309.12307.pdf) |  [Code](https://github.com/dvlab-research/LongLoRA)
- LongqLoRA: Efficient and effective method to extend context length of large language models | [arXiv 2311](https://arxiv.org/pdf/2311.04879.pdf) |  [Code](https://github.com/yangjianxin1/LongQLoRA)
- With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation | [arxiv 2401](https://arxiv.org/abs/2401.11504)
- Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning | [arxiv 2402](https://arxiv.org/abs/2402.18865)
 
### 3. Theoretical Studies
- Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning | [arXiv 2401](https://arxiv.org/pdf/2401.04151.pdf)
- The Expressive Power of Low-Rank Adaptation | [arXiv 2310](https://arxiv.org/pdf/2310.17513.pdf) |  [Code](https://github.com/UW-Madison-Lee-Lab/Expressive_Power_of_LoRA)
- LoRA Training in the NTK Regime has No Spurious Local Minima | [arXiv 2402](https://arxiv.org/pdf/2402.11867.pdf) |  [Code](https://github.com/UijeongJang/LoRA-NTK)
- NOLA: Networks as linear combination of low rank random basis | [arXiv 2310](https://arxiv.org/pdf/2310.02556.pdf) |  [Code](https://github.com/UCDvision/NOLA)
- ROSA: Random Orthogonal Subspace Adaptation | [ICML](https://openreview.net/pdf?id=4P9vOFpb63) |  [Code](https://github.com/marawangamal/rosa)
- A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA | [arXiv 2312](https://arxiv.org/pdf/2312.03732.pdf)
- Asymmetry in Low-Rank Adapters of Foundation Models | [arXiv 2402](https://arxiv.org/abs/2402.16842)


### Applications

#### LoRA in NLP
- Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning | [arXiv 2310](https://arxiv.org/pdf/2310.02954.pdf)
- Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA [ACL](https://aclanthology.org/2023.wmt-1.43.pdf)
- Task-Agnostic Low-Rank Adapters for Unseen English Dialects | [ACL](https://aclanthology.org/2023.emnlp-main.487.pdf) |  [Code](https://github.com/zedian/hyperLoRA)
- LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training | [arXiv 2401](https://arxiv.org/pdf/2401.04348.pdf) |  [Code](https://github.com/VinAIResearch/LAMPAT)

#### LoRA in CV
- Efficient low-rank backpropagation for vision transformer adaptation | [arXiv 2309](https://arxiv.org/pdf/2309.15275.pdf)
- Melo: Low-rank adaptation is better than fine-tuning for medical image diagnosis | [arXiv 2311](https://arxiv.org/pdf/2311.08236.pdf) |  [Code](https://github.com/JamesQFreeman/LoRA-ViT)
- FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers | [arXiv 2401](https://arxiv.org/pdf/2401.01752.pdf)
- Parameter-efficient Model Adaptation for Vision Transformers | [arXiv 2203](https://arxiv.org/pdf/2203.16329.pdf) |  [Code](https://github.com/eric-ai-lab/PEViT)
- ConvLoRA and AdaBN based Domain Adaptation via Self-Training | [arXiv 2402](https://arxiv.org/pdf/2402.04964.pdf) |  [Code](https://github.com/aleemsidra/ConvLoRA)
- Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks | [arXiv 2112](https://arxiv.org/pdf/2112.06825.pdf) |  [Code](https://github.com/ylsung/VL_adapter)
- Motion style transfer: Modular low-rank adaptation for deep motion forecasting | [arXiv 2211](https://arxiv.org/pdf/2211.03165.pdf) |  [Code](https://github.com/vita-epfl/motion-style-transfer)
- Enhancing General Face Forgery Detection via Vision Transformer with Low-Rank Adaptation | [arXiv 2303](https://arxiv.org/pdf/2303.00917.pdf)
- Customized Segment Anything Model for Medical Image Segmentation | [arXiv 2]
- Block-wise LoRA: Revisiting Fine-grained LoRA for Effective Personalization and Stylization in Text-to-Image Generation
    
#### LoRA for Code

- LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning | [arXiv 2308](https://arxiv.org/pdf/2308.11148.pdf)
- Task Arithmetic with LoRA for Continual Learning | [arXiv 2311](https://arxiv.org/pdf/2311.02428.pdf)

### LoRA for SCI
- X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

### LoRA for PDE
- PIHLoRA: Physics-informed hypernetworks for low-ranked adaptation [NeurIPS 2023](https://openreview.net/pdf?id=kupYlLLGdf)
   
### LoRA for Speech
- Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition | [arXiv 2309](https://arxiv.org/pdf/2309.15223.pdf)
- Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection | [arXiv 2306](https://arxiv.org/pdf/2306.05617.pdf)
- Sparsely Shared LoRA on Whisper for Child Speech Recognition | [arXiv 2309](https://arxiv.org/pdf/2309.11756.pdf)

### Federated Learning
- SLoRA: Federated parameter efficient fine-tuning of language models | [arXiv 2308](https://arxiv.org/pdf/2308.06522.pdf)
- pFedLoRA: Model-heterogeneous personalized federated learning with LoRA tuning | [arXiv 2310](https://arxiv.org/pdf/2310.13283.pdf)
- Improving LoRA in Privacy-preserving Federated Learning [OpenReview](https://openreview.net/pdf?id=NLPzL6HWNl)
- Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models | [arXiv 2401](https://arxiv.org/pdf/2401.06432.pdf)
- OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning ｜ [arXiv 2402](https://arxiv.org/pdf/2402.06954.pdf) | [Code](https://github.com/rui-ye/OpenFedLLM)
- Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning | [arXiv 2309](https://arxiv.org/abs/2309.00363)


### LoRA for Stable Diffusion
- Lcm-LoRA: A universal stable-diffusion acceleration module | [arXiv 2311](https://arxiv.org/pdf/2311.05556.pdf) |  [Code](https://github.com/luosiallen/latent-consistency-model)

### Anomaly Detection
- Parameter-Efficient Log Anomaly Detection based on Pre-training model and LoRA [Zenodo](https://zenodo.org/records/8270065)
   
### RL
- Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent | [arXiv 2402](https://arxiv.org/pdf/2402.13717.pdf) |  [Code](https://github.com/weiyifan1023/Neeko)

    
### Resource 
- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models | [arXiv 2304](https://arxiv.org/pdf/2304.01933.pdf)
- Tied-LoRA: Enhancing parameter efficiency of LoRA with weight tying | [arXiv 2311](https://arxiv.org/pdf/2311.09578.pdf) |  [Code](https://github.com/NVIDIA/NeMo/tree/adithyare/vera)
- Run LoRA Run: Faster and Lighter LoRA Implementations | [arXiv 2312](https://arxiv.org/pdf/2312.03415.pdf)
- Large language model LoRA specifically fine-tuned for medical domain tasks | [Code](https://huggingface.co/nmitchko/medfalcon-40b-LoRA)


### Multi-Task Learning
- MultiLoRA: Democratizing LoRA for Better Multi-Task Learning | [arXiv 2311](https://arxiv.org/pdf/2311.11501.pdf)

### Full-rank Projection
- GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection | [arXiv 2403](https://arxiv.org/abs/2403.03507)

### LoRA for Pretraining
- Training Neural Networks from Scratch with Parallel Low-Rank Adapters | [arXiv 2402](https://arxiv.org/pdf/2402.16828.pdf) |  [Code](https://github.com/minyoungg/LTE)
