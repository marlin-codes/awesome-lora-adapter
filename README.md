## Low-rank adapter

### LORA Method

1. Bayesian Low-rank Adaptation for Large Language Models [arXiv 2308](https://arxiv.org/abs/2308.13111)
1. DoRA: Weight-Decomposed Low-Rank Adaptation [arXiv 2402](https://arxiv.org/pdf/2402.09353.pdf)
1. LoRA+: Efficient Low-Rank Adaptation of Large Models [arXiv 2402](https://arxiv.org/pdf/2402.12354.pdf), [code](https://github.com/nikhil-ghosh-berkeley/loraplus)
1. Qlora: Efficient finetuning of quantized llms [arXiv 2305](https://arxiv.org/pdf/2305.14314.pdf), [code](https://github.com/artidoro/qlora)
1. Qa-Lora: Quantization-aware low-rank adaptation of large language models [arXiv 2309](https://arxiv.org/pdf/2309.14717.pdf), [code](https://github.com/yuhuixu1993/qa-lora)
1. Loftq: Lora-fine-tuning-aware quantization for large language models [arXiv 2310](https://arxiv.org/pdf/2310.08659.pdf), [code](https://github.com/yxli2123/LoftQ)
1. LoRA-FA: Memory-efficient low-rank adaptation for large language models fine-tuning [arXiv 2308](https://arxiv.org/pdf/2308.03303.pdf)
1. Lcm-lora: A universal stable-diffusion acceleration module [arXiv 2311](https://arxiv.org/pdf/2311.05556.pdf), [code](https://github.com/luosiallen/latent-consistency-model)
1. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation [arXiv 2210](https://arxiv.org/pdf/2210.07558.pdf), [code](https://github.com/huawei-noah/Efficient-NLP/tree/main/DyLoRA)
1. Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning [arXiv 2311](https://arxiv.org/pdf/2311.12023.pdf), [code](https://github.com/HanGuo97/lq-lora)
1. Batched Low-Rank Adaptation of Foundation Models [arXiv 2312](https://arxiv.org/pdf/2312.05677.pdf)
1. LoTR: Low Tensor Rank Weight Adaptation [arXiv 2402](https://arxiv.org/pdf/2402.01376.pdf), [code](github.com/skolai/lotr)
1. Flora: Low-Rank Adapters Are Secretly Gradient Compressors [arXiv 2402](https://arxiv.org/pdf/2402.03293.pdf), [code](https://github.com/MANGA-UOFA/Flora)
1. ELoRA: Efficient Low-Rank Adaptation with Random Matrices
1. BiLoRA: A Bi-level Optimization Framework for Low-rank Adapters [ICLR](https://openreview.net/pdf?id=Svy1XoOLXj)
1. Structure-Aware Low-Rank Adaptation for Parameter-Efficient Fine-Tuning
2. PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation [arXiv 2401](https://arxiv.org/pdf/2401.11316.pdf)
3. Hydra: Multi-head low-rank adaptation for parameter efficient fine-tuning [arXiv 2309](https://arxiv.org/pdf/2309.06922.pdf), [code](https://github.com/extremebird/Hydra)
4. Training Neural Networks from Scratch with Parallel Low-Rank Adapters [arXiv 2402](https://arxiv.org/pdf/2402.16828.pdf), [code](https://github.com/minyoungg/LTE)
5. QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning [arXiv 2402](https://arxiv.org/pdf/2402.10462.pdf)
6. ReLoRA: High-Rank Training Through Low-Rank Updates [arXiv 2307](https://arxiv.org/pdf/2307.05695.pdf), [code](https://github.com/guitaricet/relora)
7. LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation [arXiv 2402](https://arxiv.org/pdf/2402.07721.pdf)
8. Ensemble of low-rank adapters for large language model fine-tuning [arXiv 2310](https://arxiv.org/pdf/2310.00035.pdf)
9. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices [arXiv 2309](https://arxiv.org/pdf/2309.02411.pdf)
10. S-lora: Serving thousands of concurrent lora adapters [arXiv 2311](https://arxiv.org/pdf/2311.03285.pdf), [code](https://github.com/S-LoRA/S-LoRA)
11. CaraServe: CPU-Assisted and Rank-Aware LoRA Serving for Generative LLM Inference [arXiv 2401](https://arxiv.org/pdf/2401.11240.pdf)
12. LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models [arXiv 2402](https://arxiv.org/pdf/2402.11417.pdf), [code](https://github.com/yifanycc/loretta)
13. Sparse Low-rank Adaptation of Pre-trained Language Models [arXiv 2311](https://arxiv.org/pdf/2311.11696.pdf), [code](https://github.com/TsinghuaC3I/SoRA)
14. LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild [arXiv 2402](https://arxiv.org/pdf/2402.09997.pdf)
15. Lorahub: Efficient cross-task generalization via dynamic lora composition [arXiv 2307](https://arxiv.org/pdf/2307.13269.pdf), [code](https://github.com/sail-sg/lorahub)
16. One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning [arXiv 2306](https://arxiv.org/pdf/2306.07967.pdf), [code](https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA)
17. Krona: Parameter efficient tuning with kronecker adapter [arXiv 2212](https://arxiv.org/pdf/2212.10650.pdf)
18. Moelora: Contrastive learning guided mixture of experts on parameter-efficient fine-tuning for large language models [arXiv 2402](https://arxiv.org/pdf/2402.12851.pdf)
19. AMAL: Meta Knowledge-Driven Few-Shot Adapter Learning [ACL](https://aclanthology.org/2022.emnlp-main.709.pdf)
20. LQER: Low-Rank Quantization Error Reconstruction for LLMs [arXiv 2402](https://arxiv.org/pdf/2402.02446.pdf)
21. Local LoRA: Memory-Efficient Fine-Tuning of Large Language Models [OpenReview](https://openreview.net/pdf?id=LHKmzWP7RN#:~:text=Our%20approach%20aims%20to%20decouple,LoRA%20on%20math%20reasoning%20tasks.)

### LongLORA
1. Longlora: Efficient fine-tuning of long-context large language models
1. Longqlora: Efficient and effective method to extend context length of large language models
1. Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning
1. PIHLoRA: Physics-informed hypernetworks for low-ranked adaptation

### Theoretical study
1. The Expressive Power of Low-Rank Adaptation
1. LoRA Training in the NTK Regime has No Spurious Local Minima
1. Autolora: A parameter-free automated robust fine-tuning framework
2. NOLA: Networks as linear combination of low rank random basis
3. ROSA: Random Orthogonal Subspace Adaptation
4. A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA

### Applications

#### LORA in NLP
1. Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning
1. Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA
1. Task-Agnostic Low-Rank Adapters for Unseen English Dialects
2. LAMPAT: Low-Rank Adaption for Multilingual Paraphrasing Using Adversarial Training

#### LORA in CV
- Efficient low-rank backpropagation for vision transformer adaptation
- Melo: Low-rank adaptation is better than fine-tuning for medical image diagnosis
- FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers
- Parameter-efficient Model Adaptation for Vision Transformers
- ConvLoRA and AdaBN based Domain Adaptation via Self-Training
- Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks
- Motion style transfer: Modular low-rank adaptation for deep motion forecasting
- Enhancing General Face Forgery Detection via Vision Transformer with Low-Rank Adaptation

#### LORA for Code

1. LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning
1. Task Arithmetic with LoRA for Continual Learning

### LORA for SCI

X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design

### LORA for Speech
Low-rank adaptation of large language model rescoring for parameter-efficient speech recognition
Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection
Sparsely Shared LoRA on Whisper for Child Speech Recognition

### Federated Learning
1. Fedlora: Model-heterogeneous personalized federated learning with lora tuning
1. Improving LoRA in Privacy-preserving Federated Learning
1. Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models
1. SLoRA: Federated parameter efficient fine-tuning of language models
2. 


### Anomaly Detection
1. Parameter-Efficient Log Anomaly Detection based on Pre-training model and LORA
   
### RL
Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent

### LORA + MOE

1. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning."arXiv preprint arXiv:2309.05444(2023).  
2. LoraMOE: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment."arXiv preprint arXiv:2312.09979(2023).  
3. Higher Layers Need More LoRA Experts."arXiv preprint arXiv:2402.08562(2024).
4. Higher Layers Need More LoRA Experts
5. LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs
6. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications
7. Lora ensembles for large language model fine-tuning

### Resource 
1. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models
1. Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying
2. Run LoRA Run: Faster and Lighter LoRA Implementations

MultiLoRA: Democratizing LoRA for Better Multi-Task Learning

